In DevOps, Kubernetes(k8's) in Realtime Senario Interview Questions and Answers:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. How do you think the company should shift it from Monolithic to Microservices?

2. Can you explain the Overview of Kubernetes Architecture?

3. Difference between Kubernetes and Docker Swarm for container orchestration?

4. What methods are available to expose Kubernetes services to external users, and how do they differ?

5. How would you deploy an application in Kubernetes to ensure zero downtime during updates?"(Rolling upadate Stratagy)"

6. Can you explain the difference between stateful and stateless applications, and how each is managed in Kubernetes?

7. How do you handle persistent data in Kubernetes, and what are Persistent Volumes (PVs) and Persistent Volume Claims (PVCs)?

8. Describe the process manually of scaling applications in Kubernetes both horizontally and vertically?

9. What are ConfigMaps and Secrets in Kubernetes, and how are they used?

10. How can you perform a rollback of a failed deployment in Kubernetes?

11. What tools and practices would you use to monitor a Kubernetes cluster and collect logs?

12. How do you implement security measures within a Kubernetes cluster?

13. Explain Kubernetes RBAC?

14. What is a Helm Chart, and how does it facilitate application deployment in Kubernetes?

15. How does Kubernetes use custom namespaces to organize cluster resources?

16. What are liveness and readiness probes in Kubernetes, and how are they used?

17. What is a Kubernetes network policy, and how does it work?

18. What is the difference between a Kubernetes Deployment and a DaemonSet?

19. How does ingress help in Kubernetes?

20. What is a Kubernetes Custom Resource Definition (CRD), and how can it be used to extend Kubernetes functionality?

21. What is the purpose of Operators?

22. You have a Kubernetes pod that is stuck in CrashLoopBackOff . How do you diagnose and fix it?

23. You need to restrict access to a Kubernetes service based on labels. How do you achieve this?

24. You want to upgrade a deployment to a new version. How do you perform a rolling update in Kubernetes?

25.  You want to limit the CPU and memory usage for a container. How do you set resource requests and limits?

26. You have a Kubernetes cluster with multiple worker nodes. One of the nodes becomes unresponsive and needs to be replaced. 
    Explain the steps you would take to replace the node without affecting the availability of applications running on the cluster?

27.  A Kubernetes pod is stuck in a "Pending" state. What could be the possible reasons, and how would you troubleshoot it?

28.  You have a Kubernetes Deployment with multiple replicas, and some pods arefailing health checks. 
     How would you identify the root cause and fix it?

29. "What are init containers? How are they used? What is a common pattern you could use them for?
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. How do you think the company should shift it from Monolithic to Microservices? 

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ Monolithic Architecture is Everything built a single unit(code base), tightly coupled like different application(front-end,backend,DB)
   run as single code base.
   Itâ€™s simple to develop but hard to scalability â€”any change requires again will do redeploying the entire system.

ğŸ‘‰ Microservices Architecture breaks the application into independent services, 
   each handling a specific function and communicating via APIs. 
   Each service can be deployed, scaled, and updated separately, making it more flexible and better fault isolation.
------------------------------------------------------------------------------------------------------------------------------------------

2. Can you explain the Overview of Kubernetes Architecture?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ Kubernetes mainly consists of two components we can say one is MASTER NODE and second one is WORKER NODE.
   so in masternode inside there are four components..., So by the name we can understood from CM it is master node so it is 

1.Kube controller manager --> Manages the multiple process which are running masternode adn it is combined all the process together
                              and let in inform the MasterNode thats wt happening so it's basically manages all the process.

2.kube API-Server         --> Its acts as front end of MasterNode so it exposes all the API of k8s to MasterNode component and
                              is responsible for like creating communication between MasterNode and WorkerNode.

3.kube scheduler          --> Schedules like work for the WorkerNode as it inside MasterNode so it will schedule work for 
                              different WorkNodes.

4.etcd (database)         --> It's basically Key-Values to store like (username&password) so if we have to store it in this inside k8s
                              then we will mainly store it inside ETCD. 
                          ---> Okay these are 4 components which are MasterNode.

And comes into WokerNode inside there are two components..,

1.Kubelet     --> The Kubelet on that Worker Node communicates with the API Server to ensure the Pods are running.

2.Kube-Proxy  --> Kube Proxy ensures networking and allows to communication between Pods and Services.
              ---> So this the basic k8s architeture.
-----------------------------------------------------------------------------------------------------------------------------------------

3. Difference between Kubernetes and Docker Swarm for container orchestration?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "Kubernetes and Docker Swarm are both container orchestration tools, but they different in complexity, scalability, and automation".

1. Kubernetes is a powerful orchestration tool and It will automatically self-healing, automated scaling, and advanced networking,
   making it perfect for large applications that need reliability and automation.

2.Docker Swarm is lightweight and easy to set up,it offers great for quick and simple deployments, 
  but it doesnâ€™t have the advanced automation and self-healing that compared to Kubernetes.
-----------------------------------------------------------------------------------------------------------------------------------------

4. What methods are available to expose Kubernetes services to external users, and how do they differ?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ In Kubernetes, a Service is a way to expose services externally and manage network access to a set of Pods.
ğŸ‘‰ Pods are temporary and (they can be created and destroyed frequently), 
    So we can use Services thats provide a stable way to communicate between applications.

1. ClusterIP can Exposes a service within the cluster with a stable internal IP address,
2. NodePort can Exposes a service on a static port on each Node in the Cluster, 
3. LoadBalancer Exposes a service externally with a public IP, Uses a cloud load balancer"(AWS)", and 
4. Ingress provides a Manages external access using HTTP/HTTPS routes with path-based routing.

ğŸ‘‰ "For production, I prefer using Ingress with an Ingress Controller for better security, TLS termination, and traffic management."

SENARIO EXPLANATION ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ClusterIP (Default) â€“ Used for internal communication within the cluster.
1ï¸âƒ£ NodePort (Basic, for Development & Testing):-
ğŸ‘‰ Exposes the service on a static port (e.g., 30000-32767) on every worker node.
ğŸ‘‰ Users access the service using NodeIP:NodePort.
âœ… Example NodePort:
------------------------------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
    - port: 80           ### Internal service port
      targetPort: 8080   ### Container port
      nodePort: 30001    ### External access port

ğŸ”¹ Access URL: http://<Node-IP>:30001
ğŸ”» Limitations: Not recommended for production (port conflicts, security issues).

2ï¸âƒ£ LoadBalancer (Best for Cloud Environment:-
ğŸ‘‰ Allocates a cloud-managed load balancer (AWS ELB, Azure LB, GCP LB).
ğŸ‘‰ Provides an external IP to distribute traffic across pods.
âœ… Example LoadBalancer:
------------------------------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: LoadBalancer
  selector:
    app: my-app
  ports:
    - port: 80
      targetPort: 8080

ğŸ”¹ Access URL: http://<External-IP>
ğŸ”» Limitations: Only works in cloud environments, costs extra.

3ï¸âƒ£ Ingress (Best for Routing & TLS)
ğŸ‘‰ Uses a single entry point (Ingress Controller) to route traffic to multiple services.
ğŸ‘‰ Supports TLS termination (HTTPS), load balancing, and path-based routing.
âœ… Example Ingress Resource:
------------------------------------------------------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
    - host: myapp.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: my-service
                port:
                  number: 80

ğŸ”¹ Access URL: https://myapp.example.com
ğŸ”» Limitations: Requires an Ingress Controller (NGINX, Traefik, Istio).
-----------------------------------------------------------------------------------------------------------------------------------------

5. How would you deploy an application in Kubernetes to ensure zero downtime during updates?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… "To ensure zero downtime while updating an application in Kubernetes,
   I would use a "Rolling Update strategy" with Deployments". Hereâ€™s how it works and steps to explain:

ğŸ”¹ "Can you explain the process of a Rolling Update Strategy"
ğŸ‘‰ "A rolling update allows Kubernetes to update applications without downtime by gradually replacing old pods with new ones. 
    It ensures minimal disruption to users and can automatically roll back if the update fails."

1ï¸âƒ£ Define a Deployment with a specified replica count to ensure multiple pods are running.
2ï¸âƒ£ Use RollingUpdate strategy (default for Deployments) to gradually replace old pods with new ones.
3ï¸âƒ£ Set maxUnavailable=1 (ensures at least 2 the current number of replicas remain available).
4ï¸âƒ£ Set maxSurge=1 or more (allows extra pods to start before terminating old ones).
5ï¸âƒ£ Use Readiness Probes to ensure new pods are only added to the service when theyâ€™re fully ready.
6ï¸âƒ£ Use Liveness Probes to monitor pod health and prevent serving broken pods.
7ï¸âƒ£ If an update fails, I can quickly rollback the deployment using:
ğŸ‘‰ kubectl rollout undo deployment <deployment-name>
âœ… Example Deployment:
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
ğŸ”¥ "This ensures that users never experience downtime during updates!"
-----------------------------------------------------------------------------------------------------------------------------------------

6. Can you explain the difference between stateful and stateless applications, and how each is managed in Kubernetes?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
In Kubernetes, applications can be either stateless or stateful,
and they are managed differently based on their data handling requirements.

ğŸ”¹ Stateless Applications:
ğŸ‘‰ Stateless applications are ones that donâ€™t need to remember anything from one request to the next request. 
   Each request is completely independent. So, once a request is finished, all the data is discarded. 
   For example, like (web servers"NGINX,APACHE" or APIs services), where the server doesn't need to remember previous interactions. 
   In Kubernetes, we manage these with Deployments because we can easily scale themâ€”if one pod fails, 
   it can be replaced without any issues. The new pod doesnâ€™t need to know about the old one.

ğŸ”¹ Stateful Applications:
ğŸ‘‰ Stateful applications, on the other hand, need to remember data between requests. 
   So, even if the pod is restarted, it should still have access to its previous data. 
   Examples include databases (e.g., MySQL, MongoDB), message queues, and other systems that maintain state between requests. 
   In Kubernetes handles these using StatefulSets, which ensures each pod gets a unique identity, 
   and we can attach persistent storage (like Persistent Volumes) so the data doesnâ€™t get lost. 
   This makes sure that even if the pod restarts, it can still find the same data.

âœ… In simple terms:
ğŸ‘‰ Stateless apps donâ€™t store data between requests and are handled by Deployments.
ğŸ‘‰ Stateful apps store data and need StatefulSets and persistent storage to function properly.
-----------------------------------------------------------------------------------------------------------------------------------------

7. How do you handle persistent data in Kubernetes, and what are Persistent Volumes (PVs) and Persistent Volume Claims (PVCs)?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"In Kubernetes, when we need to store data that should not be lost even if a pod restarts, 
we use Persistent Volumes (PVs) and Persistent Volume Claims (PVCs).

ğŸ”¹ Persistent Volume (PV) â€“ A storage resource in the cluster that is provisioned by an admin or dynamically created. 
   It acts like a hard drive that Kubernetes can use.

ğŸ”¹ Persistent Volume Claim (PVC) â€“ A request made by a pod to use storage resource. 
   The pod doesnâ€™t directly ask for a disk; instead, it asks for a PVC, and Kubernetes matches it with an available PV.

How Persistent Storage Works in Kubernetes in pod:
1ï¸âƒ£ Create a PV â€“ Defines storage (e.g., EBS in AWS, NFS, etc.).
âœ… Example PV YAML (for a local storage volume):
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 5Gi     ------------------------> Defines the storage capacity
  accessModes:
    - ReadWriteOnce  -------------------------> Defines how the volume can be accessed (RWO, RWX, ROX)
  persistentVolumeReclaimPolicy: Retain  -----> What happens when PVC is deleted (Retain, Recycle, Delete)
  storageClassName: manual  ------------------> Storage class that matches the PVC
  hostPath:
    path: "/mnt/data"    ---------------------> Path to local storage on the node
---------------------------------------------------------------------------------------
2ï¸âƒ£ Create a PVC â€“ A pod requests storage by creating a claim.
âœ… Example PVC YAML:
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce  ------------> Should match PV access mode
  resources:
    requests:
      storage: 5Gi   ------------> Should match PV capacity
  storageClassName: manual ------> Must match the PV storage class
---------------------------------------------------------------------------------------
3ï¸âƒ£ Attach the PVC to a Pod â€“ Kubernetes binds the PVC to a suitable PV, and the pod gets persistent storage.
Example Pod using PVC:
---
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  volumes:
    - name: my-storage
      persistentVolumeClaim:
        claimName: my-pvc  ------> This references the PVC created earlier
  containers:
    - name: my-container
      image: nginx
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: my-storage

ğŸ‘‰ Even if the pod restarts, the data remains because it is stored outside the pod.

âœ… Example: Imagine a MySQL database running in Kubernetes. If the pod restarts without persistent storage, all data is lost. 
    But with a PVC attached to a PV, the database can retain its data across restarts.
    So, PVs provide actual storage, and PVCs let pods claim that storage. This ensures data persistence in Kubernetes!"
-----------------------------------------------------------------------------------------------------------------------------------------

8. Describe the process manually of scaling applications in Kubernetes both horizontally and vertically?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "To manually scale a Kubernetes Deployment, I can use kubectl scale deployment <name> --replicas=<count> for horizontal scaling. 
    Alternatively, I can modify the Deployment YAML file and set the desired number of replicas. 
    If vertical scaling is needed, I update the CPU and memory requests/limits in the pod spec and apply the changes.
    This ensures efficient resource utilization based on workload demands."

ğŸ‘‰ "In Kubernetes, there are two ways to scale an application: Horizontally and Vertically".
ğŸ”¹ Horizontal Scaling (Increasing/Decreasing Replicas):-
1ï¸âƒ£ used kubctl cmd To manually increase or decrease the number of replicas:
ğŸ‘‰ kubectl scale deployment <deployment-name> --replicas=<desired-count>
âœ… Example: Scale to 5 replicas
ğŸ‘‰ kubectl scale deployment my-app --replicas=5
âœ… To verify the scaling:
ğŸ‘‰ kubectl get deployment my-app
2ï¸âƒ£ Modify the replicas field in the Deployment YAML file:
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 5  ###### Change this value
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: my-app-image:latest
---------------------------------------------------------------
Apply the changes:
ğŸ‘‰ kubectl apply -f deployment.yaml

ğŸ”¹ Vertical Scaling (Increasing CPU & Memory for Pods):-
1ï¸âƒ£ Check Current Resource Limits
ğŸ‘‰ kubectl describe deployment <deployment-name>
2ï¸âƒ£ Update Resource Requests and Limits Modify the resources section in the Deployment YAML:
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  template:
    spec:
      containers:
        - name: my-container
          image: my-app:v1
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
----------------------------------------------------------
Apply the changes:
ğŸ‘‰ kubectl apply -f deployment.yaml
-----------------------------------------------------------------------------------------------------------------------------------------

9. What are ConfigMaps and Secrets in Kubernetes, and how are they used?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"In ConfigMaps and Secrets are used to store configuration data separately from application code, 
 making applications more flexible and secure."

ğŸ”¹ ConfigMaps (For Non-Sensitive Data):
ğŸ‘‰ Stores non-sensitive configuration data such as environment variables, config files, or command-line arguments.
ğŸ‘‰ Helps keep applications configurable without changing container images.
ğŸ‘‰ Example use case: Storing database connection URLs, API keys (if not sensitive), or log levels.
1ï¸âƒ£ Example Create a ConfigMap from YAML:
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  APP_ENV: "production"
  LOG_LEVEL: "info"

Apply the ConfigMap:
ğŸ‘‰ kubectl apply -f configmap.yaml
2ï¸âƒ£ Using a ConfigMap in a Pod:
---
apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  containers:
    - name: my-container
      image: nginx
      envFrom:
        - configMapRef:
            name: my-config

ğŸ‘‰ The pod loads environment variables APP_ENV=production and LOG_LEVEL=info from my-config.

ğŸ”¹ Secrets (For Sensitive Data):
ğŸ‘‰ Stores confidential data like passwords, API keys, TLS certificates.
ğŸ‘‰ Data is base64-encoded (not encrypted, so use RBAC to protect it).
ğŸ‘‰ Example use case: Storing database passwords, authentication tokens, or SSH keys.
1ï¸âƒ£ Create a Secret from YAML:
---
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  db_password: cGFzc3dvcmQ=   ----> (base64 encoded "password")

Apply the Secret:
ğŸ‘‰ kubectl apply -f secret.yaml

2ï¸âƒ£ Use Secret in a Pod:
---
apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  containers:
    - name: my-container
      image: nginx
      env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: DB_PASSWORD

ğŸ‘‰ The pod loads DB_PASSWORD from my-secret. It prevents hardcoding credentials in YAML files.
-----------------------------------------------------------------------------------------------------------------------------------------

10. How can you perform a rollback of a failed deployment in Kubernetes?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"In Kubernetes, if a deployment update fails, we can easily roll back to a previous stable version using kubectl rollout undo."

ğŸ”¹ Steps to Perform a Rollback
1ï¸âƒ£ Check the Deployment History
ğŸ‘‰ kubectl rollout history deployment <deployment-name>
ğŸ’¡ This shows the revision history of the deployment.

2ï¸âƒ£ Rollback to the Previous Version
ğŸ‘‰ kubectl rollout undo deployment <deployment-name>
ğŸ’¡ This reverts the deployment to the last working state.

3ï¸âƒ£ Rollback to a Specific Revision (if needed)
ğŸ‘‰ kubectl rollout undo deployment <deployment-name> --to-revision=2
ğŸ’¡ This allows you to roll back to a specific version from the history.

4ï¸âƒ£ Check the Status After Rollback
ğŸ‘‰ kubectl get pods
ğŸ‘‰ kubectl describe deployment <deployment-name>
ğŸ’¡ Ensure the rollback was successful and pods are running correctly.

ğŸ”¹ Example Use Case:-
"Imagine you deployed a new version of an application, but it caused downtime. 
 Instead of debugging immediately, you quickly roll back to the previous working version to restore service, 
 then troubleshoot the issue separately."
" It helps maintain high availability and quick recovery from failures."
-----------------------------------------------------------------------------------------------------------------------------------------

11. What tools and practices would you use to monitor a Kubernetes cluster and collect logs?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "For monitoring Kubernetes, I use Prometheus & Grafana to track CPU, memory, and pod health checks and etc,,.
     For logging, I use ELK (Elasticsearch, Logstash, Kibana) to collect and analyze logs.
     I also set up Kube-State-Metrics for cluster insights and Node Exporter for node-level monitoring. 
     This helps ensure cluster stability and quick troubleshooting."

ğŸ”¹ Monitoring Tools & Practices
âœ… Prometheus + Grafana (Most Common)
ğŸ‘‰ Prometheus: Collects metrics from Kubernetes (CPU, memory, network, etc.).
ğŸ‘‰ Grafana: Visualizes the metrics with dashboards.
ğŸ‘‰ How it works? Prometheus scrapes metrics from Kubernetes components and Grafana displays them.

ğŸ’¡ Command to check resource usage:
kubectl top pod
kubectl top node

ğŸ”¹ Logging Tools & Practices
âœ… Fluentd + Elasticsearch + Kibana (EFK Stack)
ğŸ‘‰ Fluentd: Collects logs from Kubernetes pods.
ğŸ‘‰ Elasticsearch: Stores and indexes logs.
ğŸ‘‰ Kibana: Provides a UI to search and analyze logs.

ğŸ’¡ Check logs of a pod:
kubectl logs <pod-name>
ğŸ’¡ Check logs of a pod running multiple containers:
kubectl logs <pod-name> -c <container-name>
ğŸ’¡ Stream logs in real-time:
kubectl logs -f <pod-name>
-----------------------------------------------------------------------------------------------------------------------------------------

12. How do you implement security measures within a Kubernetes cluster?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1ï¸âƒ£ RBAC (Role-Based Access Control) â€“ Restricts user and service access based on roles.
2ï¸âƒ£ Network Policies   â€“ Controls pod-to-pod and external communication, preventing unauthorized access within the cluster.
3ï¸âƒ£ Container Security â€“ Ensures containers are running securely by using image scanning tools (like Trivy).
4ï¸âƒ£ Secrets Management â€“ Stores sensitive data (like API keys and passwords), securely using Kubernetes Secrets 
                        instead of plain text environment variables.
5ï¸âƒ£ Audit Logging      â€“ Tracks all Kubernetes API activities, helping detect and respond to security incidents.
6ï¸âƒ£ Update & Patching  â€“ Regularly updating Kubernetes components, nodes, and dependencies to prevent vulnerabilities.

ğŸ”¹ "These measures provide a solid foundation for securing Kubernetes clusters."
-----------------------------------------------------------------------------------------------------------------------------------------
13. Explain Kubernetes RBAC?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "RBAC (Role-Based Access Control) in Kubernetes restricts access to resources based on user roles.
    You define Roles with specific permissions (e.g., create, delete) and bind them to users or groups using RoleBindings.

ğŸ‘‰  RBAC ensures users and applications have access only to necessary resources, 
     following the principle of least privilege and enhancing security."
-----------------------------------------------------------------------------------------------------------------------------------------

14. What is a Helm Chart, and how does it facilitate application deployment in Kubernetes?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "A Helm Chart is a package that contains all the Kubernetes resources needed to deploy an application, 
    such as Deployments, Services, and ConfigMaps. 

ğŸ‘‰ It simplifies the process by allowing you to deploy an entire application with a single command. 

ğŸ‘‰ Helm charts can be customized with a values.yaml file, like replicas, resource limits, etc,,.
    and they support versioning, making it easy to manage updates and rollbacks of applications in Kubernetes."
-----------------------------------------------------------------------------------------------------------------------------------------

15. How does Kubernetes use custom namespaces to organize cluster resources?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "In Kubernetes, namespaces are used to organize and isolate resources within a cluster. 

ğŸ‘‰ They provide a way to divide cluster resources into different environments, teams, or applications.

ğŸ‘‰ This makes it easier to manage and maintain your resources within the cluster,
   and also provides better security and resource isolation, while ensuring they donâ€™t interfere with each other."
-----------------------------------------------------------------------------------------------------------------------------------------

16. What are liveness and readiness probes in Kubernetes, and how are they used?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Liveness Probe:
ğŸ‘‰ Liveness Probe checks if the container is still running. If it fails, the container will be restarted.

ğŸ‘‰ You can set up a liveness probe to check if your app responds on a specific endpoint (like /healthz),
   if the app doesnâ€™t respond correctly, Kubernetes will restart it.
âœ… Example: Liveness Probe with HTTP Check:
---
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 10

ğŸ‘‰ Kubernetes sends an HTTP GET request to /health every 10 seconds.
ğŸ‘‰ If it fails, the pod is restarted.

ğŸ”¹ Readiness Probe:
ğŸ‘‰ Readiness Probe checks if the container is ready to handle traffic. If it fails, Kubernetes will stop routing traffic to that pod.
ğŸ‘‰ A readiness probe can be configured to check if the database is ready before allowing the app to accept traffic.
âœ… Example: Readiness Probe with HTTP Check
---
readinessProbe:
  httpGet:
    path: /readiness
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 15

ğŸ‘‰ If /ready endpoint fails, the pod is temporarily removed from the Service.
ğŸ‘‰ Once healthy again, Kubernetes adds it back.
-----------------------------------------------------------------------------------------------------------------------------------------

17. What is a Kubernetes network policy, and how does it work?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "A Kubernetes Network Policy controls pod-to-pod and external traffic, similar to a firewall. 
    By default, all pods can communicate, but network policies restrict traffic using rules based on pod labels and IP ranges. 
    This enhances security and isolates workloads."

ğŸ”¹ Key Concepts
1ï¸âƒ£ Default Behavior (Before Network Policies)
ğŸ‘‰ By default, Kubernetes allows all pod-to-pod communication within a cluster.
ğŸ‘‰ No restrictions unless you explicitly configure them.

2ï¸âƒ£ Purpose of Network Policies
ğŸ‘‰ To restrict traffic to specific pods or services.
ğŸ‘‰ Enhance security by controlling which pods can talk to each other.

3ï¸âƒ£ How It Works
ğŸ‘‰ A network policy defines a set of rules that allow or deny traffic to/from pods based on labels, namespaces, ports, and IP blocks.
ğŸ‘‰ If a policy is applied, it restricts traffic to only what is allowed by the policy and blocks anything not explicitly allowed.

4ï¸âƒ£ Network Policy Selector
ğŸ‘‰ PodSelector: Selects the pods that are affected by the policy (e.g., all pods with label app: frontend).
ğŸ‘‰ Ingress and Egress Rules: Controls incoming and outgoing traffic.

ğŸ”¹ Types of Network Policies:-
ğŸ‘‰ Ingress Policies: Controls incoming traffic to a pod.
ğŸ‘‰ Egress Policies: Controls outgoing traffic from a pod.
-----------------------------------------------------------------------------------------------------------------------------------------

18. What is the difference between a Kubernetes Deployment and a DaemonSet?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Kubernetes Deployment:
ğŸ‘‰ Purpose: Manages a replicated application that needs to run in multiple pods for scalability and high availability.
ğŸ‘‰ Behavior: Ensures that a specified number of replicas of a pod are running at any given time. 
   It provides automated rollouts, scaling, and rollback capabilities.
ğŸ‘‰ Ideal for stateless applications where you want multiple copies of your app running for high availability.

ğŸ”¹ Kubernetes DaemonSet
ğŸ‘‰ Purpose: Ensures that a copy of a pod is running on every node (or a subset of nodes) in the cluster.
ğŸ‘‰ Behavior: Automatically creates a pod on each node in the cluster, ensuring that a single instance of the pod runs on each node.
ğŸ‘‰ No scaling involved â€” there will always be one pod per node.

ğŸ‘‰ "A Kubernetes Deployment ensures a specified number of replicas of a pod are running, ideal for stateless applications." 
ğŸ‘‰ "A DaemonSet ensures that a pod runs on every node in the cluster, making it ideal for system-level tasks like monitoring or logging."
-----------------------------------------------------------------------------------------------------------------------------------------

19. How does ingress help in Kubernetes?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "Ingress in Kubernetes is used to manage external access to the services inside the cluster, typically HTTP/HTTPS traffic. 
   It provides features like load balancing, SSL termination, and path-based or host-based routing. 
   Ingress helps simplify service exposure by providing a single entry point and managing traffic routing efficiently, 
   improving scalability and security in your cluster."

ğŸ”¹ Example Use Case:
ğŸ‘‰ "A company hosting multiple microservices (app.example.com, api.example.com) can use Ingress to expose them 
    a single Ingress Controller instead of creating separate Load Balancers, reducing costs and improving security."
-----------------------------------------------------------------------------------------------------------------------------------------

20. What is a Kubernetes Custom Resource Definition (CRD), and how can it be used to extend Kubernetes functionality?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "A Kubernetes Custom Resource Definition (CRD) allows you to define custom resources in your cluster, 
   extending Kubernetes with new resource types. 
   CRDs are commonly used with controllers or operators to automate complex tasks and manage application-specific resources.
   This gives you the flexibility to build Kubernetes-native solutions for your needs, 
   while maintaining the benefits of Kubernetes' declarative configuration and scalability."

ğŸ”¹ Example Use Case:
ğŸ‘‰ "A company using Kafka in Kubernetes can define a KafkaCluster CRD, 
     It allowing teams to deploy and manage Kafka clusters just like native Kubernetes resources, simplifying operations."
-----------------------------------------------------------------------------------------------------------------------------------------

21. What is the purpose of Operators?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "A Kubernetes operator is a method of packaging, deploying, and managing a Kubernetes application.
   An operator uses the Kubernetes API to automate tasks such as deployment,
   scaling, implementing custom controllers, enabling self-healing systems, and supporting declarative management practices."

ğŸ”¹ Example Use Case:
ğŸ‘‰ "Prometheus Operator automates the deployment and management of Prometheus monitoring instances on Kubernetes, 
    handling tasks such as configuration, scaling, and self-healing, thereby simplifying the monitoring of Kubernetes clusters."

-----------------------------------------------------------------------------------------------------------------------------------------

22. You have a Kubernetes pod that is stuck in CrashLoopBackOff. How do you diagnose and fix it?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "A CrashLoopBackOff in Kubernetes means that a pod is repeatedly crashing and restarting due to an error. 
    I follow a structured approach to diagnose and resolve the issue."

1. Check pod logs to understand why itâ€™s crashing:
ğŸ‘‰ kubectl logs <pod-name> --previous
ğŸ”¹ Example:
ğŸ‘‰ The --previous flag retrieves logs from the previous instance of the container before it restarted.
ğŸ‘‰ This helps identify application errors, missing dependencies, or misconfigurations that caused the crash.

2. Describe the pod to get detailed information about its state and events:
ğŸ‘‰ kubectl describe pod <pod-name>
ğŸ”¹ Example:
ğŸ‘‰ This command provides detailed insights about the pod, including:
ğŸ‘‰ Recent events (e.g., failed liveness probes, OOMKilled errors).
ğŸ‘‰ Container status (e.g., CrashLoopBackOff, ImagePullBackOff).
ğŸ‘‰ Reason for failure (e.g., health check failures, permission issues).

3. After checking logs and kubectl describe, common issues could be:
âœ… Misconfigurations (e.g., wrong port, missing environment variables).
âœ… Missing Files/Dependencies (e.g., required config files not mounted).
âœ… Incorrect Image Versions (e.g., using an incompatible or broken Docker image).
âœ… Resource Limits Exceeded (e.g., pod getting OOMKilled due to memory shortage).

4. Fix the issue based on the logs and events:
âœ… If an environment variable is missing: Update the deployment YAML.
âœ… If the wrong image is used: Update the correct image and redeploy.

5. To restart the pod manually after applying fixes:
ğŸ‘‰ kubectl delete pod <pod-name>
-----------------------------------------------------------------------------------------------------------------------------------------

23. You need to restrict access to a Kubernetes service based on labels. How do you achieve this?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "To restrict access to a Kubernetes service based on labels, I use NetworkPolicies to control traffic at the pod level."

1ï¸âƒ£ Use a NetworkPolicy to Restrict Access:
ğŸ‘‰ A NetworkPolicy defines rules that allow or deny traffic between pods based on labels.
ğŸ”¹ Example Scenario:
ğŸ‘‰ Service A should only be accessed by Pods with label app=frontend.
ğŸ‘‰ Other pods in the cluster should not reach Service A.

2ï¸âƒ£ Define the NetworkPolicy (YAML Example):
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: restrict-access
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: service-a  --------> Targeting the pods running Service A
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: frontend  --------> Only allow traffic from frontend pods
      ports:
        - protocol: TCP
          port: 80  --------> Define the service port

3ï¸âƒ£ How It Works?
âœ… The podSelector selects pods running Service A (label: app=service-a).
âœ… The ingress rule allows access only from pods with app=frontend.
âœ… Other pods cannot access Service A, effectively restricting access based on labels.

4. Apply the network policy:
ğŸ‘‰ kubectl apply -f <networkpolicy.yaml>
-----------------------------------------------------------------------------------------------------------------------------------------

24. You want to upgrade a deployment to a new version. How do you perform a rolling update in Kubernetes?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "In Kubernetes, a rolling update ensures zero downtime by gradually replacing old pods with new ones."

1ï¸âƒ£ Update the Deployment with the New Version:
ğŸ”¹ Modify the container image in the deployment and apply the changes:
ğŸ‘‰ kubectl set image deployment/<deployment-name> <container-name>=<new-image>:<tag>
ğŸ”¹ Example: Updating nginx deployment to version 1.21:
ğŸ‘‰ kubectl set image deployment/nginx-deployment nginx=nginx:1.21

2ï¸âƒ£ Verify the Rolling Update Progress:
ğŸ”¹ Monitor the update to ensure new pods are replacing old ones gradually:
ğŸ‘‰ kubectl rollout status deployment/<deployment-name>

ğŸ‘‰ Kubernetes will automatically perform a rolling update by replacing the old pods with the new ones without downtime.
-----------------------------------------------------------------------------------------------------------------------------------------

25. You want to limit the CPU and memory usage for a container. How do you set resource requests and limits?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "To control resource usage in Kubernetes, I set CPU and memory requests and limits in the podâ€™s deployment YAML and apply the changes."
ğŸ‘‰ Requests: Kubernetes uses these values to schedule the pod (ensures the pod gets enough resources).
ğŸ‘‰ Limits: Kubernetes enforces these limits (pod will be killed if it exceeds them).

1ï¸âƒ£ Define Resource Requests and Limits in YAML:
ğŸ”¹ Modify the resources section inside the pod or deployment YAML file.
ğŸ”¹ Example: Increase memory limit to 1Gi (512Mi) and CPU 500m (250m):
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-container
          image: my-app:latest
          resources:
            requests:
              memory: "512Mi"  --------> Minimum memory requested
              cpu: "250m"      --------> Minimum guaranteed CPU (0.25 cores)
            limits:
              memory: "1Gi"    --------> Maximum memory limit (previously lower)
              cpu: "500m"      --------> Max CPU (0.5 cores)

2ï¸âƒ£ Apply the Changes:
ğŸ‘‰ kubectl apply -f pod.yaml

or, if modifying an existing deployment:
ğŸ‘‰ kubectl set resources deployment my-app --limits=cpu=500m,memory=512Mi --requests=cpu=250m,memory=256Mi

3ï¸âƒ£ Verify the Resource Limits:
ğŸ‘‰ kubectl describe pod <pod-name>
-----------------------------------------------------------------------------------------------------------------------------------------

26. You have a Kubernetes cluster with multiple worker nodes. One of the nodes becomes unresponsive and needs to be replaced. 
   Explain the steps you would take to replace the node without affecting the availability of applications running on the cluster?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "To replace an unresponsive Kubernetes node without downtime, I first drain the node to safely evict running pods. 
    Then, I delete the node from the cluster and provision a new node. 
    If using a cloud provider, Auto Scaling Groups can handle this automatically. 
    Finally, I add the new node back to the cluster and verify that workloads are running correctly. 
    This ensures high availability of applications without disruption."
-----------------------------------------------------------------------------------------------------------------------------------------

27. A Kubernetes pod is stuck in a "Pending" state. What could be the possible reasons, and how would you troubleshoot it?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "When a Kubernetes pod is stuck in a 'Pending' state, it means the pod is unable to be scheduled on any node."
    I follow a structured troubleshooting approach to identify and resolve the issue.

1ï¸âƒ£ Check Pod Status:
ğŸ‘‰ kubectl get pods -o wide
âœ… If a pod is stuck in "Pending", it usually means Kubernetes cannot schedule it on a node.

2ï¸âƒ£ Describe the Pod for More Details:
ğŸ‘‰ kubectl describe pod <pod-name>
ğŸ”¹ Look for:
ğŸ‘‰ Events: Section (Scheduling errors, insufficient resources, or taints).
ğŸ‘‰ Conditions: Reasons why the pod is not running.

3ï¸âƒ£ Check for Resource Constraints:
ğŸ‘‰ kubectl describe node <node-name>
ğŸ”¹ Possible issues:
ğŸ‘‰ Nodes have insufficient CPU/memory.
ğŸ‘‰ Limits or requests in resources: section are too high.

4ï¸âƒ£ Check for Node Selectors & Taints:
ğŸ”¹ If a pod has node selectors or tolerations, it may not find a suitable node.
ğŸ‘‰ kubectl describe pod <pod-name>
ğŸ‘‰ kubectl describe node <node-name>
ğŸ”¹ Possible issues:
ğŸ‘‰ The pod has a nodeSelector that doesn't match any available node.
ğŸ‘‰ Nodes are tainted, and the pod lacks the required tolerations.

5ï¸âƒ£ Check if the Scheduler is Working Properly:
ğŸ”¹ If the Kubernetes scheduler is not functioning, no new pods will be scheduled.
ğŸ‘‰ kubectl get pods -n kube-system | grep scheduler

6ï¸âƒ£ Check if Persistent Volume (PV) is Available:
ğŸ”¹ If a pod is waiting for a volume to be bound, it remains in "Pending".
ğŸ‘‰ kubectl get pvc
ğŸ”¹ Possible issue:
ğŸ‘‰ No available PersistentVolume for the requested claim.

7ï¸âƒ£ Force Reschedule the Pod:
ğŸ”¹ If the issue is still unresolved, delete the pod and let Kubernetes reschedule it.
ğŸ‘‰ kubectl delete pod <pod-name>
-----------------------------------------------------------------------------------------------------------------------------------------

28.  You have a Kubernetes Deployment with multiple replicas, and some pods are failing health checks. 
     How would you identify the root cause and fix it?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‘‰ "When multiple pods in a Kubernetes Deployment are failing health checks, 
    it indicates an issue with the application, configuration, or underlying infrastructure."
     I follow a systematic approach to diagnose and resolve the issue.

1ï¸âƒ£ Identify Check pod status in the Deployment:
ğŸ‘‰ kubectl get pods -l app=<app-name>
âœ… If some pods show CrashLoopBackOff or Error, proceed with further investigation.

2ï¸âƒ£ Inspect Pod Logs for Errors to check why the application is failing:
ğŸ‘‰ kubectl logs <pod-name>
ğŸ”¹ For multi-container pods:
ğŸ‘‰ kubectl logs <pod-name> -c <container-name>
ğŸ”¹ Common issues:
ğŸ‘‰ Application crashes due to misconfiguration or dependency failures.
ğŸ‘‰ Database connection failures or incorrect environment variables.

3ï¸âƒ£ Describe the Pod for More Details:
ğŸ‘‰ kubectl describe pod <pod-name>
ğŸ”¹ Check for:
ğŸ‘‰ Events section: Shows failed liveness/readiness probes.
ğŸ‘‰ Container state: Waiting or Terminated reasons.
ğŸ‘‰ Resource limits: If the pod is being OOMKilled (Out of Memory).

4ï¸âƒ£ Check liveness and readiness Health Probe Configurations in View Deployment YAML:
ğŸ‘‰ kubectl get deployment <deployment-name> -o yaml
ğŸ”¹ Possible probe issues:
ğŸ‘‰ Incorrect path or port in readinessProbe / livenessProbe.
ğŸ‘‰ Probes timing out before the app fully starts.

5ï¸âƒ£ Check Resource Limits & Node Constraints:
ğŸ‘‰ kubectl describe pod <pod-name>
ğŸ”¹ Possible issues:
ğŸ‘‰ Insufficient CPU/memory causing crashes.
ğŸ‘‰ NodeSelector or Taints preventing scheduling.

6ï¸âƒ£ If the issue is resolved, restart the failing pods:
ğŸ‘‰ kubectl delete pod <pod-name>
ğŸ”¹ Or restart the Deployment:
ğŸ‘‰ kubectl rollout restart deployment <deployment-name>
-----------------------------------------------------------------------------------------------------------------------------------------

29. "What are init containers? How are they used? What is a common pattern you could use them for?

EXPLAIN IN THIS ANSWER:-
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Œ Init Containers in Kubernetes:
ğŸ”¹ Init containers are specialized containers that run before the main application containers start in a pod.
ğŸ”¹ They are used for initialization tasks such as setting up configurations, checking dependencies, or preparing the environment.
ğŸ”¹ Unlike regular containers, init containers always run to completion first, and only then do the main containers start.

ğŸ“Œ How Are Init Containers Used:
âœ… They are defined in the PodSpec under initContainers along with the main containers.
âœ… Each init container must complete successfully before the next one starts.
ğŸ‘‰ Example YAML Configuration:
---
apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  initContainers:
  - name: init-db
    image: busybox
    command: ["sh", "-c", "until nc -z db-service 3306; do echo waiting for database; sleep 2; done"]
  containers:
  - name: app
    image: my-app-image
    ports:
    - containerPort: 8080

ğŸ”¹ In this example, the init container init-db:
ğŸ‘‰ Waits until the database service (db-service:3306) is reachable before starting the main application.












